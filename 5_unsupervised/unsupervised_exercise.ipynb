{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Module 5: Unsupervised"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setup the matplotlib styling\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # Try to use the BI style sheet for plots\n",
    "    line1 = (0/256, 224/256, 170/256)\n",
    "    line2 = (96/256, 126/256, 229/256)\n",
    "    line3 = (136/256, 76/256, 255/256)\n",
    "    plt.style.use('matplotlibrc')\n",
    "    plt.rcParams['axes.prop_cycle'] = plt.cycler(color=[(136/256, 76/256, 255/256), (60/256, 170/256, 207/256), (12/256, 229/256, 177/256)]) \n",
    "    \n",
    "    colors = [(0.53125, 0.296875, 0.99609375), (0.453125, 0.3984375, 0.9453125), (0.375, 0.4921875, 0.89453125), (0.3046875, 0.578125, 0.8515625), (0.234375, 0.6640625, 0.80859375), (0.16015625, 0.75390625, 0.76171875), (0.09375, 0.8359375, 0.72265625), (0.046875, 0.89453125, 0.69140625), (0.0, 0.875, 0.6640625)]\n",
    "    bicmap = LinearSegmentedColormap.from_list(name='BIcmp', \n",
    "                                                colors=colors,\n",
    "                                                N=len(colors))\n",
    "    cm_bright = ListedColormap([(0.53125, 0.296875, 0.99609375), (12/256, 229/256, 177/256)])\n",
    "    colors = np.array([line1, line2, line3])\n",
    "except:\n",
    "    bicmap = plt.cm.BuGn \n",
    "    colors = ['r', 'g', 'b']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 5.1: Clustering**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply different cluster algorithms on the data set in the data folder:  Use k-means, gaussian mixture models und hierarchical clustering."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv('data/blobs.csv')\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the scatterplot function of the pandas module to visualize the results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Currently we are using only one color of the colors array\n",
    "data.plot.scatter(x='feature_1', y='feature_2', color=colors[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.1.1: KMeans**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize the KMeans algorithm\n",
    "# TODO\n",
    "# Fit the model on the data\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the prediction of the algorithm with scatter() by passing the the predicted cluster assignment to the colors array\n",
    "# Tip: colors[kmeans.predict(data)]\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**How do we choose the correct number of clusters (when we have more than 3 features)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to choose the number of clusters in a way that they maximize the silhouette score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import silhouette_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Log all scores\n",
    "s_score = []\n",
    "# Test each number of clusters d from 2 to 20\n",
    "num_clusters = list(range(2, 20+1))\n",
    "\n",
    "for d in num_clusters:\n",
    "    # Intialize a new KMeans model with n_clusters=d\n",
    "    # TODO\n",
    "    # Fit the KMeans model\n",
    "    # TODO\n",
    "    # Calculate the silhouette score with (data, kmeans.predict(data))\n",
    "    score = None # TODO\n",
    "    # append the calculated score to the list\n",
    "    s_score.append(score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot the result and the select the number of clusters\n",
    "plt.plot(num_clusters, s_score)\n",
    "\n",
    "plt.xlabel('#clusters')\n",
    "plt.ylabel('silhouette_score')\n",
    "plt.gca().set_xticks(num_clusters)\n",
    "plt.gca().set_xticklabels(num_clusters)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Rerun your code with the optimal number of clusters**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.1.2: Gaussian Mixture**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Intialize the GaussianMixture model with n_components=3\n",
    "# TODO\n",
    "# Fit the model on the data\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the prediction as you have done before.\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.1.3: Hierarchical Clustering**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Helper function to plot the dendogram\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize the AgglomerativeClustering algorithm with distance_threshold=0, n_clusters=None\n",
    "model = None # TODO\n",
    "# Fit the clustering algorithm\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, truncate_mode='level')\n",
    "plt.xlabel(\"Data points\")\n",
    "plt.gca().set_xticklabels([])\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Based on the dendogram, how many clusters should you choose?**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3! Because for the three remaining clustering algorithms we have the largest distance until the next merge happens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Rerun the code. This time specify the number of clusters and plot the result**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize the AgglomerativeClustering algorithm with n_clusters=? (?=your chosen number of clusters)\n",
    "# TODO\n",
    "# You don't need to fit the algorithm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the prediction as you have done before.\n",
    "# This time instead of predict use fit_predict(data)\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.1.4: DBSCAN**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cluster import DBSCAN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize dbscan\n",
    "# TODO\n",
    "# You don't need to fit the algorithm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Adds a new color for outliers\n",
    "extended_colors = np.append(colors, [[0, 0, 0]], axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the prediction as you have done before.\n",
    "# This time instead of predict use fit_predict(data)\n",
    "# Use extended_colors to visualize outliers\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Change the hyperparameters so that DBSCAN finds all three clusters, while ingoring the outliers.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 5.2: Dimensionality Reduction**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Have a look at the digits dataset. What does the data set contain? Perform PCA and MDS on digits set and visualize result."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.imshow(digits[\"images\"][0], cmap=\"Greys\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(digits[\"images\"][13], cmap=\"Greys\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.2.1: PCA**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Intialize PCA with n_components=2\n",
    "# TODO\n",
    "# Project the digits into the two dimensional space using fit_transform(digits.data)\n",
    "projected = None # TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the results\n",
    "plt.figure()\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=bicmap)\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.2.2: MDS**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.manifold import MDS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Intialize mds with n_components=2\n",
    "# TODO\n",
    "# Reduce the number of dimensions with fit_transform\n",
    "projected = None # TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the results\n",
    "plt.figure()\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=bicmap)\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.2.3: t-SNE**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.manifold import TSNE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize tsne with n_components=2\n",
    "# TODO\n",
    "# Reduce the number of dimensions with fit_transform\n",
    "projected = None # TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the results\n",
    "plt.figure()\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=bicmap)\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 5.3: Outlier Detection**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to find abnormal transactions in credit card payments. For that you find data for the credit card transactions of a person in the file data/credit_card_data.csv. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transactions = pd.read_csv('data/credit_card_data.csv')\n",
    "transactions.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.3.1**\n",
    "\n",
    "Visualize the data using pandas scatter. Do you have an explanation for what you see? Which transactions do you think are outliers?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transactions.plot.scatter(x='AMOUNT_SPEND', y='DISTANCE_TO_HOME')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.3.2**\n",
    "\n",
    "Find outliers using the Isolation Forest algorithm. Is the result as you would expect?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize an IsolationForest object\n",
    "# TODO\n",
    "# Fit the model on the data\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the outlier score\n",
    "# To visualize the probability of being an outlier from isolationforest use\n",
    "# cmap=bicamp and c=isolationforest.predict(transactions)\n",
    "# TODO\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Is the result what you expected?**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.3.3**\n",
    "\n",
    "What could you do to handle the outliers in the two separate regions of the separately? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Think about how you could transform the data to get a more intuitive result. Hint: The logarithm might help."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Intialize the a kmeans object with (n_clusters=2)\n",
    "# TODO\n",
    "# Fit kmeans on the transactions\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Based on the kmeans algorithm devide the data into two chunks\n",
    "# Tip: You can index rows in a DataFrame with transactions[cond]\n",
    "# Tip: You can create conditions by performing boolean operations on a numpy array\n",
    "\n",
    "# Save the predicted cluster assignment of kmeans in a new variable\n",
    "# TODO\n",
    "\n",
    "# Create the condition prediction == 0 and save it in a variable\n",
    "# TODO\n",
    "# Index transactions with the condition and save the view on the transactions in a new variable\n",
    "# TODO\n",
    "# We also need to scale the transactions in this chunk with np.log()\n",
    "# TODO\n",
    "\n",
    "# Create the condition prediction == 1 and save it in a variable\n",
    "# TODO\n",
    "# Index transactions with the condition and save the view on the transactions in a new variable\n",
    "# TODO\n",
    "# We also need to scale the transactions in this chunk with np.log()\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit one isolation forest on each chunk of transactions\n",
    "# This time also add contamination=0.002 to the construction of the isolation forest object\n",
    "\n",
    "# Initialize the isolationforest0\n",
    "# TODO\n",
    "# Fit the isolationforest0 on the first chunk\n",
    "# TODO\n",
    "\n",
    "# Intialize the isolationforest1\n",
    "# TODO\n",
    "# Fit the isolationforest1 on the second chunk\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Finally, visualize the results\n",
    "# Tip: Use the code you used before for plotting\n",
    "# Tip: To connect both plots save the ax object returend from the first .plot.scatter() and then pass it to the second .plot.scatter(ax=ax)\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Do the results now make more sense?**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.3.4**\n",
    "\n",
    "New transcations come in. They are given in the file data/credit_card_new_transactions.csv. How would you detect whether these are abnormal or not?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_transactions = pd.read_csv('data/credit_card_new_transactions.csv')\n",
    "new_transactions.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# First separate the samples into the two clusters\n",
    "# TODO\n",
    "\n",
    "# Create the condition new_prediction == 0 and save it in a variable\n",
    "# TODO\n",
    "# Index new_transactions with the condition and save the view on the new_transactions in a new variable\n",
    "# TODO\n",
    "# We also need to scale the new_transactions in this chunk with np.log\n",
    "# TODO\n",
    "\n",
    "# Create the condition new_prediction == 1 and save it in a variable\n",
    "# TODO\n",
    "# Index new_transactions with the condition and save the view on the new_transactions in a new variable\n",
    "# TODO\n",
    "# We also need to scale the new_transactions in this chunk with np.log\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Classify the samples as outliers or not\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 5.4**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the cancer data set from sklearn."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "cancer_data = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "cancer_data['target'] = cancer.target\n",
    "cancer_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Have a look at the data. How many samples do you have? How many features? What are the features? What is the target? Is this a classification or regression problem?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the function pairplot of seaborn to visualize the components Worst Texture, Worst Symmetry, Mean Concave Points and Mean radius together. What do you notice?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = ['worst texture', 'worst symmetry', 'mean concave points', 'mean radius']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.plotting.scatter_matrix(cancer_data[features], figsize=(10, 10))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.4.1**\n",
    "\n",
    "Perform a PCA on the data set. Remember: PCA needs normalization before applying it in a useful manner. Use sklearn.preprocessing.StandardScaler to normalize the data. Then perform a PCA with sklearn.decomposition.PCA and three components. Print out the explained variances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 5.4.2**\n",
    "Visualize the first two components together with the target. What do you notice?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('bi_deep_learning-M-dGzlNK': pipenv)"
  },
  "interpreter": {
   "hash": "3a851e0f79b59b2bccaf1ffc2243b1acd0ae10df96b3f2ce733c4d83b39092de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}