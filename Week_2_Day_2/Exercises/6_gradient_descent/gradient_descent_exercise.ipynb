{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Module 6: Gradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setup the matplotlib styling\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # Try to use the BI style sheet for plots\n",
    "    line1 = (0/256, 224/256, 170/256)\n",
    "    line2 = (96/256, 126/256, 229/256)\n",
    "    line3 = (136/256, 76/256, 255/256)\n",
    "    plt.style.use('matplotlibrc')\n",
    "    plt.rcParams['axes.prop_cycle'] = plt.cycler(color=[(136/256, 76/256, 255/256), (60/256, 170/256, 207/256), (12/256, 229/256, 177/256)]) \n",
    "    \n",
    "    colors = [(0.53125, 0.296875, 0.99609375), (0.453125, 0.3984375, 0.9453125), (0.375, 0.4921875, 0.89453125), (0.3046875, 0.578125, 0.8515625), (0.234375, 0.6640625, 0.80859375), (0.16015625, 0.75390625, 0.76171875), (0.09375, 0.8359375, 0.72265625), (0.046875, 0.89453125, 0.69140625), (0.0, 0.875, 0.6640625)]\n",
    "    bicmap = LinearSegmentedColormap.from_list(name='BIcmp', \n",
    "                                                colors=colors,\n",
    "                                                N=len(colors))\n",
    "    cm_bright = ListedColormap([(0.53125, 0.296875, 0.99609375), (12/256, 229/256, 177/256)])\n",
    "    colors = np.array([line1, line2, line3])\n",
    "except:\n",
    "    bicmap = plt.cm.BuGn \n",
    "    colors = ['r', 'g', 'b']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 6.1**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The gradient descent method is an algorithm that minimizes a function f. It works like this:\n",
    "- Initialize x to some value  \n",
    "- While “still change”:  \n",
    "    x=x -f'(x)  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 6.1.1**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to implement the gradient descend method in python for the function f(x)=x^4. Use x=5 as starting point."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is the function we want to minimize\n",
    "def func(x):\n",
    "    return x**4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the derivative of f(x)\n",
    "def gradient_f(x):\n",
    "    # TODO\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Our starting point will be x=5\n",
    "x = 5\n",
    "# Append all tried positions to this list\n",
    "xpos = []\n",
    "\n",
    "for i in range(5):\n",
    "    # Use the gradient function to calculate the gradient\n",
    "    # TODO\n",
    "    # Use the update rule from above to change the next x based on the gradient\n",
    "    # TODO\n",
    "    # Log the tried x to xpos\n",
    "    xpos.append(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot the results\n",
    "\n",
    "# Create the target function\n",
    "x_lin = np.linspace(-10, 10, 20)\n",
    "f_lin = x_lin**4\n",
    "\n",
    "# Calculate the y value for all tried points\n",
    "ypos = [func(x) for x in xpos]\n",
    "\n",
    "plt.plot(x_lin, f_lin, color=line1, label='Original function')\n",
    "plt.scatter(xpos, ypos, color=line2, label='Checked points')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Does this strategy lead you to the minimal point 0?**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What is the problem and what do you need to change?**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 6.1.2**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Use the code from above, but this time include a learning rate\n",
    "\n",
    "# Our starting point will be x=5\n",
    "x = 5\n",
    "# Append all tried positions to this list\n",
    "xpos = []\n",
    "\n",
    "for i in range(5):\n",
    "    # Use the gradient function to calculate the gradient\n",
    "    # TODO\n",
    "    # Use the update rule from above to change the next x based on the gradient this time with a learning rate\n",
    "    # TODO\n",
    "    # Log the tried x to xpos\n",
    "    xpos.append(x)\n",
    "\n",
    "# Plot the results\n",
    "x_lin = np.linspace(-10, 10, 20)\n",
    "f_lin = x_lin**4\n",
    "print(xpos)\n",
    "ypos = [func(x) for x in xpos]\n",
    "\n",
    "plt.plot(x_lin, f_lin, color=line1, label='Original function')\n",
    "plt.scatter(xpos, ypos, color=line2, label='Checked points')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Does this strategy lead you to the minimal point 0?**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 6.2: Linear Regression**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to implement our own Linear Regression modul with gradient descent."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 6.2.1: One-dimensional linear regression**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the dataset\n",
    "reg_data = pd.read_csv('data/LinearRegression.csv')\n",
    "x = reg_data['x'].values\n",
    "y = reg_data['y'].values\n",
    "reg_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the linear regression as \n",
    "def f(x, intercept, slope):\n",
    "    return intercept + slope * x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate the error as the difference between the desired output and the predicted output\n",
    "def error(desired_y, predicted_y):\n",
    "    # TODO\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the loss as the squared sum of errors\n",
    "def squared_loss(error):\n",
    "    # TODO\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the gradients of our function\n",
    "# Returning one gradient for intercept and one gradient for slope separated by a comma\n",
    "def gradient_f(error, x):\n",
    "    # TODO\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Our starting point will\n",
    "intercept = 0\n",
    "slope = 0\n",
    "\n",
    "for i in range(10):\n",
    "    # Make a prediction with the function f\n",
    "    pred_y = None # TODO\n",
    "    # Calculate the error of the prediction\n",
    "    e = None # TODO\n",
    "    # Calculate the loss\n",
    "    l = None # TODO\n",
    "    # Calculate the gradient\n",
    "    # Tip: The actual gradient is mean of the gradients from all data points\n",
    "    grad_intercept, grad_slope = None # TODO\n",
    "    # Update the parameters based on your gradients\n",
    "    # Tip: Use the learning rate of 0.1 for the intercept and 0.01 for the slope\n",
    "    # TODO\n",
    "\n",
    "    # Plot the results of the iteration\n",
    "    x_lin = np.linspace(x.min(), x.max(), 40)\n",
    "    f_lin = f(x_lin, intercept, slope)\n",
    "\n",
    "    plt.plot(x_lin, f_lin, color=line1, label='Predicted Linear Regression')\n",
    "    plt.scatter(x, y, color=line2, label='Data points')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 6.2.2: Doing something different**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For your above optimization process, change the update rule from\n",
    "```python\n",
    "theta = theta - lr * gradient\n",
    "```\n",
    "to\n",
    "```python\n",
    "theta = theta + lr * gradient\n",
    "```\n",
    "\n",
    "**What happens when you do that?**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Bonues Exercise 6.2.3: Mulitdimensional Linear Regression**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modify your code to allow optimizing any dimension."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "x, y = make_regression(n_features=20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the linear regression as \n",
    "def f(x, params):\n",
    "    # TODO\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the gradient function\n",
    "def gradient_f(error, x):\n",
    "    # TODO\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Our starting point will be random\n",
    "params = np.random.random((1, x.shape[1] + 1))\n",
    "\n",
    "for i in range(30):\n",
    "    # Make a prediction with the function f\n",
    "    y_pred = None # TODO\n",
    "    # Calculate the error of the prediction\n",
    "    e = None # TODO\n",
    "    # Calculate the loss\n",
    "    l = None # TODO\n",
    "    # Calculate the gradient\n",
    "    gradients = None # TODO\n",
    "    # Update the parameters based on your gradients\n",
    "    # TODO\n",
    "\n",
    "    # We can't plot 100 dimensions\n",
    "    # Print the loss and see if it decreases\n",
    "    print(l)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 6.3: Logistic Regression**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clf_data = pd.read_csv('data/LogisticRegression.csv')\n",
    "x = clf_data['x'].values\n",
    "y = clf_data['y'].values\n",
    "z = clf_data['z'].astype(int).values\n",
    "clf_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 6.3.1: Two dimensional logistic regression**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the logistic model\n",
    "def logistic_regression(x, y, intercept, slope1, slope2):\n",
    "    return 1/(1+np.e ** -(intercept + slope1 * x + slope2 * y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the gradient function\n",
    "# Return separated by comma the gradients for intercept, slope1, slope2\n",
    "def gradient_f(error, x, y, output):\n",
    "    # TODO\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Our starting point will\n",
    "intercept = 0\n",
    "slope1 = 0\n",
    "slope2 = 0\n",
    "\n",
    "for i in range(50+1):\n",
    "    # Make a prediction with the function f\n",
    "    pred_y = None # TODO\n",
    "    # Calculate the error of the prediction\n",
    "    e = None # TODO\n",
    "    # Calculate the loss\n",
    "    l = None # TODO\n",
    "    # Calculate the gradient\n",
    "    grad_intercept, grad_slope1, grad_slope2 = None # TODO\n",
    "    # Update the parameters based on your gradients\n",
    "    # Tip: A learning rate of 0.2 should work\n",
    "    # TODO\n",
    "    \n",
    "    # Every tenthed interation visualize the results\n",
    "    if i % 10 == 0:\n",
    "        # Visualize each iteration and and the loss\n",
    "        h = 0.2 # Resolution\n",
    "        xx, yy = np.meshgrid(np.arange(x.min()-0.5, x.max()+0.5, h), np.arange(y.min()-0.5, y.max()+0.5, h))\n",
    "        zz = logistic_regression(xx.ravel(), yy.ravel(), intercept, slope1, slope2).reshape(xx.shape)\n",
    "\n",
    "        plt.contourf(xx, yy, zz, cmap=bicmap, alpha=.8)\n",
    "        plt.scatter(x, y, c=colors[1-z], cmap=bicmap, edgecolors='k')\n",
    "        plt.title(f'Iteration {i}: Loss {l}')\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Bonus Exercise 6.3.2: Multi-dimensional Logistic Regression**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Redefine the data for this\n",
    "x = clf_data[['x', 'y']].values\n",
    "y = clf_data['z'].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the logistic model\n",
    "def logistic_regression(x, params):\n",
    "    # TODO\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the gradient function\n",
    "def gradient_f(error, x, output):\n",
    "    # TODO\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Our starting point will\n",
    "params = np.random.random((1, x.shape[1] + 1))\n",
    "\n",
    "for i in range(100):\n",
    "    # Make a prediction with the function f\n",
    "    pred_y = None # TODO\n",
    "    # Calculate the error of the prediction\n",
    "    e = None # TODO\n",
    "    # Calculate the loss\n",
    "    l = None # TODO\n",
    "    # Calculate the gradient\n",
    "    gradients = None # TODO\n",
    "    # Update the parameters based on your gradients\n",
    "    # TODO\n",
    "\n",
    "    # We can't plot more than 3 dimensions\n",
    "    # Print the loss and see if it decreases\n",
    "    print(l)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 6.4: Categorical Loss**\n",
    "\n",
    "Why would we prefer to use the this loss function instead of squared error: \n",
    "```\n",
    "l(y,pred_y) = -[y log(y_pred) + (1-y) log⁡(1-y_pred)] \n",
    "```"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('bi_deep_learning-M-dGzlNK': pipenv)"
  },
  "interpreter": {
   "hash": "3a851e0f79b59b2bccaf1ffc2243b1acd0ae10df96b3f2ce733c4d83b39092de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}