{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Module 2: Complex Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise we will apply the machine learning models, we saw in todays exercise.\n",
    "We will visualize the learned rules of the models with the `plot_decision_boundaries()` or `plot_regression()` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "try:\n",
    "    # Try to use the BI style sheet for plots\n",
    "    plt.style.use('matplotlibrc')\n",
    "    plt.rcParams['axes.prop_cycle'] = plt.cycler(color=[(136/256, 76/256, 255/256), (60/256, 170/256, 207/256), (12/256, 229/256, 177/256)]) \n",
    "    \n",
    "    colors = [(0.53125, 0.296875, 0.99609375), (0.453125, 0.3984375, 0.9453125), (0.375, 0.4921875, 0.89453125), (0.3046875, 0.578125, 0.8515625), (0.234375, 0.6640625, 0.80859375), (0.16015625, 0.75390625, 0.76171875), (0.09375, 0.8359375, 0.72265625), (0.046875, 0.89453125, 0.69140625), (0.0, 0.875, 0.6640625)]\n",
    "    bicmap = LinearSegmentedColormap.from_list(name='BIcmp', \n",
    "                                                colors=colors,\n",
    "                                                N=len(colors))\n",
    "    cm_bright = ListedColormap([(0.53125, 0.296875, 0.99609375), (12/256, 229/256, 177/256)])\n",
    "except:\n",
    "    bicmap = plt.cm.BuGn \n",
    "    colors = ['r', 'g', 'b']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 2.1**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this part of the exercise we use a regression problem and test different models and compare how well they fit the true target function.\n",
    "\n",
    "Explain the fit of the different models.\n",
    "Which model would you use to solve this particular problem?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reg_data = pd.read_csv('data_regression.csv')\n",
    "reg_features = reg_data[['x']].values\n",
    "reg_target = reg_data['y'].values\n",
    "reg_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the dataset\n",
    "reg_data.plot.scatter(x='x', y='y')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Helper function to plot the learned function and compare it to the true function\n",
    "def plot_regression(rg, features, target):\n",
    "    x_lin = np.linspace(features.min(), features.max(), 100)\n",
    "\n",
    "    plt.scatter(features, rg.predict(features), color=(136/256, 76/256, 255/256), s=19, marker='o', label=\"prediction\")\n",
    "    plt.plot(x_lin, rg.predict(x_lin.reshape(-1, 1)), color=(60/256, 170/256, 207/256), linewidth=2, label=\"smooth prediction\")\n",
    "    plt.scatter(features, target, color=(12/256, 229/256, 177/256), s=19, marker='o', label=\"y\")\n",
    "    plt.plot(x_lin, np.sin(x_lin), color=(12/256, 229/256, 177/256), linewidth=2, label=\"true function\")\n",
    "\n",
    "\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 2.1.1: Linear Regression**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the LinearRegression model\n",
    "from sklearn.linear_model import LinearRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize the LinearRegression model\n",
    "lr = LinearRegression()\n",
    "# Fit the model on the data\n",
    "lr.fit(reg_features, reg_target)\n",
    "# Plot the learned function\n",
    "plot_regression(lr, reg_features, reg_target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 2.1.2: Polynomial Regression**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the PolynomialFeatures class\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Intialize the PolynomialFeatures\n",
    "pf = PolynomialFeatures()\n",
    "# Use fit_perform to convert data into polynomial features\n",
    "pf_features = pf.fit_transform(reg_features)\n",
    "pf_features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize a LinearRegression model\n",
    "poly_reg = LinearRegression()\n",
    "# Train the LinearRegression model on the polynomial features\n",
    "poly_reg.fit(pf_features, reg_target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# With this setup we can't use plot_regression(), because the function poly_reg use different datasets\n",
    "# Instead can create a pipeline that takes the original dataset and converts it into polynomial feature space before feeding it to the linear regression model\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Use make_pipeline() to create a pipeline of PolynomialFeatures() and LinearRegression() by passing the instances of these two classes to make_pipeline()\n",
    "pipe = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "# The pipeline can now be treated as a LinearRegression model\n",
    "# Fit the pipeline with the original regression dataset\n",
    "pipe.fit(reg_features, reg_target)\n",
    "# Pass the pipeline to the plot_regression() function together with the original regression dataset\n",
    "plot_regression(pipe, reg_features, reg_target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Try to adjust the number of polynomials with PolynomialFeatures(x), where x is maximum degree of polynomial features.**\n",
    "Which degree fits the data best?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipe = make_pipeline(PolynomialFeatures(5), LinearRegression())\n",
    "pipe.fit(reg_features, reg_target)\n",
    "plot_regression(pipe, reg_features, reg_target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 2.1.3: SVM**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the Support Vector Regressor (SVR)\n",
    "from sklearn.svm import SVR"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize the SVR model\n",
    "svr = SVR()\n",
    "# Fit the Support Vector Machine\n",
    "svr.fit(reg_features, reg_target)\n",
    "# Plot the learned regression function\n",
    "plot_regression(svr, reg_features, reg_target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Exercise 2.2**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit each of the models and visualize the decision boundary. How does the theory behind the learning algorithm explain the decision boundaries you see in the plots?\n",
    "\n",
    "Finally, decide which model you would use to solve this particular problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "# Split into feature columns\n",
    "features = data[['x', 'y']].values\n",
    "# And the target column\n",
    "target = data['z'].astype(int).values\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the dataset\n",
    "data.plot.scatter(x='x', y='y', c=data['z'], colormap=cm_bright, colorbar=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Helper function for plotting the decision boundaries of a trained model\n",
    "def plot_decision_boundaries(clf, x, y):\n",
    "    h = 0.02  \n",
    "\n",
    "    x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5\n",
    "    y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                            np.arange(y_min, y_max, h))\n",
    "\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=bicmap, alpha=.8)\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright, edgecolors='k')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 2.2.1: Decision Tree**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize the DecisionTree\n",
    "dt = DecisionTreeClassifier()\n",
    "# Fit the decision tree on the data\n",
    "dt.fit(features, target)\n",
    "# Plot the DecisionTree boundaries\n",
    "plot_decision_boundaries(dt, features, target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 2.2.2: RandomForest**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "# Fit the random forest on the data\n",
    "rf.fit(features, target)\n",
    "# Plot the random forest decision boundaries\n",
    "plot_decision_boundaries(rf, features, target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 2.2.3: Gradient Boosting**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize the GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "# Fit the gradient boosting model on the data\n",
    "gb.fit(features, target)\n",
    "# Plot the ensemble decision boundaries\n",
    "plot_decision_boundaries(gb, features, target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 2.2.4: Multi-layer Perceptron**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize the MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "# Fit the neural network on the data\n",
    "mlp.fit(features, target)\n",
    "# Plot the mlp decision boundaries\n",
    "plot_decision_boundaries(mlp, features, target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Bonus**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are a couple of models we haven't applied yet."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Apply the following models to the regression problem:\n",
    "- DecisionTreeRegressor (from sklearn.tree import DecisionTreeRegressor)\n",
    "- Mulit-layer Perceptron Regressor (from sklearn.neural_network import MLPRegressor)\n",
    "- Random Forest Regressor (from sklearn.ensemble import RandomForestRegressor)\n",
    "- Gradient Boosting Regressor (from sklearn.ensemble import GradientBoostingRegressor)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "reg_names = [\n",
    "    'Decision Tree',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting',\n",
    "    'MLP'\n",
    "]\n",
    "\n",
    "reg_models = [\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    MLPRegressor()\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for name, model in zip(reg_names, reg_models):\n",
    "    model.fit(reg_features, reg_target)\n",
    "    plt.title(name)\n",
    "    plot_regression(model, reg_features, reg_target)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply the following models to the classification problem we used previously\n",
    "- Logistic Regression (from sklearn.linear_model import LogisticRegression)\n",
    "- Support Vector Classifier (SVC) (from sklearn.svm import SVC)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also apply the models to a number of different datasets and see how the results differ.\n",
    "Apply the above models to the following datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.datasets import make_classification, make_circles, make_moons\n",
    "\n",
    "x, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)\n",
    "x += 2 * np.random.default_rng(2).uniform(size=x.shape)\n",
    "dataset2 = (x, y)\n",
    "\n",
    "dataset3 = make_moons(noise=0.3, random_state=0)\n",
    "\n",
    "dataset4 = make_circles(noise=0.2, factor=0.5, random_state=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\n",
    "    'Logistic Regression',\n",
    "    'Polynomial Logistic Regression',\n",
    "    'Decision Tree',\n",
    "    'SVM',\n",
    "    'Random Forest',\n",
    "    'Gradient Boosting'\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    make_pipeline(PolynomialFeatures(), LogisticRegression()),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "    ]\n",
    "\n",
    "datasets = [(features, target), dataset2, dataset3, dataset4]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# redefine the plot_decision_boundaries function to include axes contexts\n",
    "def plot_decision_boundaries_2(clf, X, y, ax):\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=bicmap, alpha=.8)\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,\n",
    "                edgecolors='k')\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    X, y = ds\n",
    "\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "\n",
    "    ax.set_xlim(X[:, 0].min(), X[:, 0].max())\n",
    "    ax.set_ylim(X[:, 1].min(), X[:, 1].max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        \n",
    "        plot_decision_boundaries_2(clf, X, y, ax)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('bi_deep_learning-M-dGzlNK': pipenv)"
  },
  "interpreter": {
   "hash": "3a851e0f79b59b2bccaf1ffc2243b1acd0ae10df96b3f2ce733c4d83b39092de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}