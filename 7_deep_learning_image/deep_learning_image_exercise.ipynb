{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Module 7: Deep Learning and Image Recognition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this exercise we will be using tensorflow. Tensorflow is a tensor computation framework. The Keras library is build on top of tensorflow to provide a much more user friendly interface.\n",
    "\n",
    "Finally, to illustrate the ease of use of pretrained deep learning models, we will apply the fastai library on medical data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "First install the necessary libraries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow tensorflow-datasets pydicom kornia opencv-python scikit-image pyarrow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import Keras and Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Datasets**\n",
    "Download the datasets, which might take a while."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This code block downloads the cats vs. dogs dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the cats vs. dogs dataset\n",
    "train_ds, validation_ds, test_ds = tfds.load(\n",
    "    \"cats_vs_dogs\",\n",
    "    # Reserve 10% for validation and 10% for test\n",
    "    split=[\"train[:20%]\", \"train[20%:30%]\", \"train[30%:40%]\"],\n",
    "    as_supervised=True,  # Include labels\n",
    ")\n",
    "\n",
    "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
    "print(\n",
    "    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\n",
    ")\n",
    "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This code loads the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 7.1: Densely connected Neural Networks**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the sequential model from keras to build a Neural Network with only densely connected layers.\n",
    "The model should be trained on the MNIST dataset and is finally evaluated on the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build a really simple densely connected neural network\n",
    "model = None # TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# look at the model with\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Next we have to compile our model\n",
    "# Choose an optimizer, a loss function and a metric to monitor\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit the model on the data\n",
    "# Tip: Keep the number of epochs low, to reduce the runtime (e.g.: 2-4 epochs should suffice for a reasonable result)\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = None # TODO\n",
    "# What is the best performance you were able to achieve?\n",
    "print(\"test loss, test acc:\", results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 7.2: Convolutional Neural Networks**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This time build a Convoluational Neural Network on the same MNIST dataset. Compare the test accuracy, training time and number of parameters between the two models. Which architecture would you use for the MNIST dataset?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The data needs to be 3D (pixel_height, pixel_width, color_channels/dimensions)\n",
    "# That's why we run the code again\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# This time make the dataset 4-D\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build a Convolutional model\n",
    "model = None # TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Look at the number of parameters with\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Again, compile the model\n",
    "# Choose an appropriate optimizer, loss function and metric to monitor\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit the model on the data\n",
    "# Tip: Again, limit the number of epoch the algorithms runs to make it comparable to the previous experiment\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Exercise 7.3: Transferlearning**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this exercise we will try to build a dog vs. cat predictor. Since the problem is much more complex than the previous one, we want to use a pre-trained image classification model.\n",
    "\n",
    "Use the pre-trained ResNet-50 model and add a new top layer to classify only dogs and cats. Train the model for a couple of epochs and evaluate it on the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Preprocessing**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a couple of the images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(train_ds.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(int(label))\n",
    "    plt.axis(\"off\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Standardization\n",
    "# Rescale and fix the image size to 150,150\n",
    "size = (150, 150)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add caching and pre-loading for speedup\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Use the pretrained ResNet-50 as a base_model\n",
    "base_model = keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    pooling='avg',\n",
    "    input_shape=(150, 150, 3)\n",
    ")\n",
    "\n",
    "# Freeze the base model, by setting base_model to not trainable\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a new input layer for our new dataset\n",
    "inputs = None # TODO\n",
    "\n",
    "# Add data augmentation\n",
    "data_augmentation = keras.Sequential(\n",
    "    [layers.RandomFlip(\"horizontal\"), layers.RandomRotation(0.1),]\n",
    ")\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# Rescale color values of 0..255 to range of -1..+1\n",
    "scale_layer = layers.Rescaling(scale=1/127.5, offset=-1)\n",
    "ax = scale_layer(x)\n",
    "\n",
    "# Put ResNet into inference mode\n",
    "x = base_model(x, training=False)\n",
    "# Add dropout to the final layer\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# Add another Dense layer on top of the base model\n",
    "outputs = None # TODO\n",
    "\n",
    "# Create the new model \n",
    "model = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print the layers with\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the top layer\n",
    "# Choose an appropriate optimizer, loss and monitoring metric\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit the model\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the model on the test set\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Bonus Exercise 7.4: Doing it the fast way**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise we use the fastai library to quickly finetune a pre-trained model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fastai.basics import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.medical.imaging import *\n",
    "\n",
    "import pydicom\n",
    "\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Downloads the DICOM files\n",
    "pneumothorax_source = untar_data(URLs.SIIM_SMALL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a train and validation split\n",
    "items = get_dicom_files(pneumothorax_source/f\"train/\")\n",
    "trn,val = RandomSplitter()(items)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can look at a sample\n",
    "patient = 7\n",
    "xray_sample = items[patient].dcmread()\n",
    "xray_sample.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read in the label information\n",
    "df = pd.read_csv(pneumothorax_source/f\"labels.csv\")\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Put the training in a DataBlock\n",
    "# That is later used as a data source for training\n",
    "pneumothorax = DataBlock(blocks=(ImageBlock(cls=PILDicom), CategoryBlock),\n",
    "                   get_x=lambda x:pneumothorax_source/f\"{x[0]}\",\n",
    "                   get_y=lambda x:x[1],\n",
    "                   batch_tfms=[*aug_transforms(size=224),Normalize.from_stats(*imagenet_stats)])\n",
    "\n",
    "dls = pneumothorax.dataloaders(df.values, num_workers=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We can plot the first Batch of 16 images\n",
    "dls = pneumothorax.dataloaders(df.values)\n",
    "dls.show_batch(max_n=16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the model\n",
    "# For fastai you don't need to specify a loss function or optimizer\n",
    "learn = cnn_learner(dls, resnet34, metrics=accuracy)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The loss function was chosen based on the dataset\n",
    "# You can have a look at it with\n",
    "learn.loss_func"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The optimzer was chosen based on the dataset\n",
    "# You can have a look at it with\n",
    "learn.opt_func"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the best learning rate\n",
    "learn.lr_find()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Finetune the model\n",
    "learn.fit_one_cycle(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show the result of a single batch\n",
    "learn.show_results(max_n=16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the performance with the confusion matrix\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(7,7))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "upp, low = interp.confusion_matrix()\n",
    "tn, fp = upp[0], upp[1]\n",
    "fn, tp = low[0], low[1]\n",
    "print(tn, fp, fn, tp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate the sensitivity\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate the specificity\n",
    "# TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate the accuracy\n",
    "#TODO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This is model considered good? How can we improve the model?**"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('bi_deep_learning-M-dGzlNK': pipenv)"
  },
  "interpreter": {
   "hash": "3a851e0f79b59b2bccaf1ffc2243b1acd0ae10df96b3f2ce733c4d83b39092de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}